\section{Overview of network application fuzzing techniques}
\label{extend:netfuzz}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem statement}
\label{extend:netfuzz:intro}

While fuzzing of stateless systems has been the norm for a while, recent years have seen a growing interest in stateful fuzzing. Specifically, network applications constitute an underrepresented category of targets that are critical to the security of both individuals and institutions. As algorithmic improvements steadily become more difficult to achieve, research also begins to shift towards analyzing previously untested systems. Combined with the fact that new bugs require exponentially more processing power to be discovered \cite{bohme2020fuzzing} in a given time frame, we expect to see significant contributions to network fuzzing in the near future.

Another consequential trend that we observe is the adoption of hypervisor-assisted techniques in defensive security solutions. After looking at recent advancements in malware analysis \cite{leon2021hypervisor,karvandi2022hyperdbg} we remark on how virtualization mechanisms such as Extended Page Tables (EPT) and hardware emulation are used not only to perform highly optimized debugging, but also to exert unprecedented levels of control over code packers and other obfuscation techniques \cite{coccia2022study,visaggio2023state}. These characteristics are also highly desirable in fuzzers. Considering how most fuzzers today are guided by code coverage, there is enough reason to try and reduce the overhead of control flow tracing instrumentation by means of hardware-assisted tracing \cite{kadar2021safety,ding2021hardware}. Moreover, the prevalence of grey-box fuzzers over that of white-box solutions can largely be attributed to their cost-effective techniques. The possibility of not only inspecting the internal state of the system in detail but to freely manipulate it at no significant cost could potentially bridge the gap between grey-box and white-box fuzzers. Finally, the ability of implementing hardware emulators for virtualized kernels could enable safe automatic testing of device drivers while also permitting tracing at memory access level.

In this section we present the state of the art network fuzzing solutions. Additionally, we take an in-depth look at the technical details of prominent fuzzer implementations, noting current tendencies of adapting them to interface with a wider class of target systems. Finally, we review existing fuzzer evaluation conventions and benchmarking frameworks. With this, we aim to provide the reader with a good understanding of the architecture of a fuzzer, of open research questions, and of the challenges that are likely to be encountered when entering this field of study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fuzzing background}
\label{extend:netfuzz:background}

In this section we first present the three principal types of fuzzers based on the amount of control they are able to exert over the target system, and their respective limitations. Then, we describe the anatomy of a fuzzer while presenting and summarily comparing the most common implementations for each component.

\subsubsection{Classification}
\label{extend:netfuzz:classification}

Broadly speaking, fuzzers can be categorized into three types depending on the amount of information available regarding the System Under Test (SUT).

\textbf{Black-box fuzzers:} These types of fuzzers operate under the harshest constraints. Without access to the internal state of the SUT, the feedback loop that influences the input mutation algorithm consists solely of the output of the target. The output is very much dependent of the target system. In IoT fuzzing, a domain where black-box fuzzing is prevalent due to restricted firmware access, solutions such as \texttt{Snipuzz} \cite{feng2021snipuzz} use the output of services that run on the SUT as indicators of code coverage and use them to guide the mutation process. In lieu of such readily available output data, research has shown \cite{sperl2019side} that side channels such as power consumption leakage may contain enough do infer code coverage.

\textbf{White-box fuzzers:} As opposed to black-box fuzzers, the white-box types have extensive access to the internals of the SUT and are able to not only monitor their state in detail, but even tamper with it. One situation where this could be desirable is if one wants to establish and enforce a strict execution path while varying the input. The reason why one may want to do this is to observe how a well defined set of branch conditions change their output based on minute changes in the SUT input. Other methods of leveraging this high degree of control may include performing taint analysis \cite{ganesh2009taint} in order to correlate regions in the input with values obtained during the execution of the program. Another white-box fuzzing technique is \textit{concolic execution} \cite{sen2007concolic}. This is a combination between concrete and symbolic execution that sidesteps the limitations of theorem provers and symbolic analyzers while also targeting hard to reach regions of code. QSYM \cite{yun2018qsym} is an example of a concolic execution engine based on dynamic instrumentation that is designed to work in tandem with \texttt{AFL}, thus creating a \textit{hybrid fuzzer}. While \texttt{AFL} has the option of spawning multiple instances that cooperate by sharing the interesting testcases among one another, QSYM pretends to be such an instance. During its execution, it steals testcases from \texttt{AFL}, identifies deeper, hard to reach branches and employs a constraint solver to generate inputs that are capable of overcoming this obstacle. These inputs are then fed back into \texttt{AFL}. Although white-box techniques are exceedingly powerful, their increased complexity ultimately renders them impractical.

\textbf{Grey-box fuzzers:} This category combines the straightforward approach of black-box fuzzers with heuristics based on limited access to the internal state of the SUT. This permits grey-box fuzzers to direct the search for new inputs based on measurements obtained during execution. While doing so, they avoid overreliance on excessively complex white-box techniques. Typically, the metrics used to guide input generation are based on code coverage \cite{bohme2016coverage,pham2019smart} and require no code analysis. Due to this fact, the focus of the fuzzer is to incrementally produce inputs that move further away from easily reachable execution paths, towards less accessible regions. This not only increases the chances of identifying a bug, but also the quality of that bug. Here, we equate "quality" with the difficulty of detection by other fuzzers or other methods entirely. When compared to their black-box and white-box counterparts, grey-box fuzzers are empirically proven to have the best performance relative to computational cost.

\subsubsection{Architecture of a fuzzer}
\label{extend:netfuzz:architecture}

A fuzzer consists of various interdependent components that are implemented with various degrees of separation.

\textbf{Test case generator:} This component is responsible for generating inputs for the SUT and usually incorporates a variety of techniques meant to maximize the the amount of code accessed during successive executions. This is referred to as \textit{code coverage}. While the ultimate goal of a fuzzer is to identify anomalous behaviour of programs (usually associated with crashes), the input mutation drivers are rarely deterministic, fact which renders the number of crashes unreliable as a metric for comparison, not only between different implementations but also between iterations of the same fuzzer. As an alternative, code coverage is both sound due to its level of correlation with produced crashes, and less prone to random influencers. The strategies used for input mutation can range from randomly flipping bits, performing arithmetic operations on certain bytes or resizing the data under consideration to performing contextually significant modifications. These modifications can be predetermined, such as setting the values of certain bytes to uncommon values (e.g., common buffer sizes, results of overflow, etc.) or informed by a user-defined grammar. The latter is highly dependant on manual analysis and comprehension of the mechanisms employed by the SUT. For example, it stands to reason that triggering different behaviours in a HTTP server is predicated by providing inputs that mostly adhere to the HTTP format. Thus, it follows that the performance of a fuzzer is tied to a set of initial inputs called \textit{seed files}. These seeds are chosen such that they guarantee the immediate discovery of a varied set of execution paths that are likely to be obtained from conventional utilization of the SUT. Consequently, mutations applied to these seeds are more likely trigger branch flips in subsequent executions than randomly selected seeds.

\textbf{Execution unit:} Once a test case is produced, it needs to be evaluated against the SUT. The execution unit is responsible for spawning an instance of the SUT and providing it with the generated input. Depending on its design, the input can be delivered as a file, data piped to the standard input, network packets, etc. If more than one delivery method are applicable, those with lesser latency are preferable (e.g., disk I/O is significantly slower than passing data via a pipe). This fact is relevant because the suitability of a fuzzer is usually expressed as a function of time. While highlighting the efficiency of a mutation strategy by expressing its evolution in comparison to the number of iterations may seem preferable, the development of fuzzers is driven by pragmatic reasons. Regardless of the sophistication of said mutation strategy, the fact remains that the limiting factor in discovering bugs via fuzzing is computational power and thus, time. If the aforementioned strategy can be outperformed by a simpler version with higher throughput, its inclusion would eventually prove detrimental. Precisely for this reason, fuzzers implement techniques for speeding up the execution of the SUT.

One solution relating to the test case generator is \textit{input minimization}. This implies reducing the size of a test case while maintaining the same coverage. \texttt{AFL} provides a separate tool specifically for this purpose, namely \texttt{\texttt{AFL}-tmin}. While it stands to reason that a smaller input takes less time to parse by the SUT, the minimization process itself is costly. As a result the time allowance of said process should be adjusted according to the potential impact of foregoing further exploration of new mutations.

Another method of reducing execution time is implementing a \textit{fork server}. Replacing the process image with a \texttt{exec()} system call is considered an expensive operation. Overwriting the memory maps of the forked process and executing the setup code leading up to the \texttt{main()} function (i.e., dynamic loader, library constructors, etc.) may take up to a few milliseconds. The fork server consists of a hook placed before the \texttt{main()} function that blocks entry until it receives a command from the fuzzer. Once such a command is received, the server forks the process and permits the child to proceed in the execution of the program while processing the current test case. This is possible due to the fact that the child inherits the file descriptors (the standard I/O descriptors are not marked \texttt{close-on-exec}). Moreover, the child process will have \texttt{copy-on-write} access to the memory pages of the parent, meaning that it will not be required to follow the same initialization procedure characteristic of an `exec()`. Finally, there are two noteworthy remarks to make regarding the fork server:

\begin{itemize}
    \item The fork server can be used at arbitrary points in the program. While initially used strictly to quickly reset the process to what was considered its initial state, the same solution can be applied at any point during the execution. On one hand, establishing such reset paths could be used to avoid repeated passes through "uninteresting" sections of code. On the other hand, executing code that does not cause context switches may prove less taxing than employing the fork server. The correct choice in this regard is highly dependent on the nature of the SUT, so this determination is best left to the user.

    \item As described until this point, the fork server is not adaptable to any situation. For example, if the SUT is a kernel module, it cannot benefit from the same functionalities that are offered to a userspace process. However, recent advancements have introduced hypervisor-assisted approaches that help expand the area of applicability of techniques such as the fork server. We explore this topic further in section \ref{extend:netfuzz:survey}.
\end{itemize}

%% Component #3
\textbf{Evaluator:} After executing the SUT on a newly generated input, the suitability of the mutation that lead to its creation must also be determined. This determination is not only required in order to set apart valuable test cases (i.e., inputs that uncover a vulnerability), but can also dictate the mutation strategies that are to be employed in future iterations. As a result, each test case is assigned a score that is usually derived from the code coverage. This requires to SUT to make information pertaining to control flow available to the fuzzer during the execution. There are several methods of achieving this:

\begin{itemize}
    \item \textit{Compile-time instrumentation:} Usually implemented as a LLVM pass, this allows the insertion of monitor code at key places in the SUT binary. Specifically, the Evaluator requires either the sequence of Basic Blocks (BB) that are executed, or the location and outcome of each `jump` instruction. The monitor establishes a communication channel with the fuzzer and reports on the state of the control flow. This method has the advantage of generating optimized code, where the monitor has low overhead. However, this requires access to the source code of the SUT. Moreover, a downside to using LLVM is that its API changes significantly between releases. Unless the fuzzer is actively maintained and the LLVM pass is updated regularly, newer compiler versions will not be compatible. This can have the effect of diminishing the pool of SUTs available for testing in the future since Open Source projects can have newer LLVM features as hard requirements. An alternative to LLVM passes is writing a wrapper over the preferred assembler and inserting the monitor as hand-written assembly code during the corresponding compilation stage. Although less flexible and harder to achieve, this solution is also more portable.

    \item \textit{Run-time instrumentation:} This method is analogous to compile-time instrumentation, the main difference being that by sacrificing performance, the requirement for having access to the source code can be disregarded. In achieving this form of instrumentation, we note Intel PIN \cite{luk2005pin}
    and \cite{bruening2003infrastructure}, two of the more well-known frameworks. While providing similar features, Intel PIN is better documented and considerably more accessible than DynamoRIO. On the other hand, DynamoRIO is OSS while Intel PIN is not. Regardless, dynamic instrumentation should be used as a last resort. As mentioned when describing the execution unit, the performance of a fuzzer is tightly linked with the amount of executions per second it can produce. Although both of the aforementioned frameworks can eventually achieve near static instrumentation levels of overhead, this requires the SUT to pass through each relevant BB at least once. The reason for this is that the frameworks translate straight-line code sequences using a JIT compiler while adding the instrumented code according to the user requirements. In fuzzing, it is highly unlikely that most code sequences are visited multiple times during an execution. As a result, the SUT instrumentation will have the worst possible overhead at every step.

    \item \textit{ptrace() instrumentation:} Comparable in performance to the previous approach, this method uses the process trace mechanism that is exposed by the kernel to debuggers such as \texttt{gdb}. Internally, tracing is performed by replacing the first byte of each breakpoint instruction with a \texttt{int 3} instruction (or the non-x86 architectural equivalent). This causes the SUT (also named "inferior" in this context) to trap into the kernel. There, control over the interrupted SUT is ceded to the tracing process (i.e. the "superior"). As a result, the SUT is guaranteed to generate repeated context switches in lieu of a JIT compiler similar to what Intel PIN and DynamoRIO employ. One situation in which this approach could potentially be viable is when having access to debug information. Since \texttt{gdb} implements both an internal Python interpreter, a case could be made for taking advantage of its DWARF parser integration. However, we note that use cases of such an implementation are highly limited. Otherwise, we note that \texttt{Honggfuzz} \cite{honggfuzz} does use the \texttt{ptrace()} API but only for determining the cause of a crash.

    \item \textit{QEMU emulation:} A common tool in performing black-box fuzzing, \texttt{QEMU} can be used either as a whole-system emulator, or as a user space emulator. The former allows running a full operating system in a virtualized environment. The latter only executes a single process, emulating its target architecture while also performing translations for system calls, POSIX signals and simulating multithreading on virtual CPUs. Depending on the nature of the SUT, either can be used. Most fuzzers that use \texttt{QEMU} for instrumentation prefer to utilize a patched version that overrides the block translation callback (i.e., the JIT component that translates a non-native BB to native code). The main advantages of this approach are the ability to test cross-compiled code and the widening of available SUTs (due to the whole system emulation).

    \item \textit{Performance Counters:} Many CPU manufacturers today include performance monitoring extensions. For example, ARM defines CoreSight: a set of programmable interfaces used to develop extensions such as the Program Flow Trace. Similarly, Intel has Processor Trace extension. Both of these systems allow tracing the execution of programs by recording accesses to certain types of instructions or the occurrences of certain events. These instructions pertain mostly to conditional jumps. Information about the jumps include their address, as well as the branch that was taken. While ARM has better support for event tracing (e.g., exceptions, synchronization primitives, hardware debug state changes, etc.) these are not usually useful in control flow monitoring. The reason why only a small subset of instructions trigger the hardware monitors is that all these events are stored in a main memory buffer. Even taking into account that the trace data is automatically compressed (a single Intel PT entry can occupy less than 1 byte), considering the frequencies at which modern CPUs operate it is reasonable to expect data streams upwards of 100MB/s. Another aspect to consider is the difficulty of interfacing with these systems. Configuring them usually requires at least EL1 / ring0 privileges. Moreover, even if mechanisms to suppress recording of instructions while in non-EL0 / ring3 modes are in place, the CPU cannot discriminate based on the currently running process. Disabling traces while a SUT is not running on any core still requires support in the operating system. To this end, we believe that employing the \texttt{perf\_events} interface may prove beneficial. The advantage of this approach is that no type of instrumentation is required and the SUT can run at normal speed. However, decompressing and interpreting the recorded data may exceed the time saved during target execution. The benefits should be evaluated on a case-by-case basis.
\end{itemize}

A second problem to investigate is what should be considered an increase in code coverage. Naturally, newly explored code should be taken into account. However, sometimes arriving at hard to reach blocks of code may be predicated on executing a number of loop iterations. \texttt{AFL}, for example, keeps track of all jumps between code blocks by allocating a 64KB buffer that is shared with the fork server, and implicitly with the executing target process. Each executed jump is codified as a tuple containing the starting address and the destination address. A hash of this tuple is then used as offset into the memory map to identify the counter keeping track of how many times a certain jump occurred. Nonetheless, not every increase in a counter is considered a significant broadening of coverage. In stead, the range of values is split in intervals (or buckets) sized 1, 4, 8, 16, 96, 128+. Only by breaking through into a new range of values can a testcase be considered to have produced an increase in coverage. Although different fuzzers could have different interpretation of what code coverage should be, a differential analysis should be performed using the same criteria. Since \texttt{AFL} offers a utility (i.e., \texttt{\texttt{AFL}-showmap}) that reports the trace bitmap of a program under a certain input, it is common to see fuzzers evaluated according to \texttt{AFL} rules, based on all the testcases they generated in a set period of time.

Another challenge when evaluating a testcase is determining whether it was able to trigger an anomalous behaviour in the SUT. While crashes are clear indicators that this is indeed the case, most bugs do not necessarily cause failures of this kind (e.g., not every buffer overflow causes a segmentation fault). Moreover, testcases that could highlight a bug do not necessarily increase code coverage, so it may be tempting to prematurely discard them as "not interesting enough" to serve as basis for future mutations. The solution here is to employ sanitizers (e.g., Address Sanitizer, Undefined Behaviour Sanitizer, Valgrind, etc.) in order to force crashes in these situation. The question is when to employ them. On one hand, the detection of bugs may serve as an important source of information when producing the following inputs. On the other hand, each sanitizer introduces significant overhead that could drastically reduce the number of executions per second. Moreover, it is not impossible to trigger bugs that are introduced by the sanitizer instrumentation which are then mistakenly attributed to the SUT. Depending on their necessity for the mutation strategy, it may prove desirable to run the fuzzer on a "clean" version of the binaries and do an offline analysis of the generated testcases on another, heavily sanitized version to identify crashes, if such a metric is considered relevant.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Challenges in network fuzzing}
\label{extend:netfuzz:challenges}

In this section we present a number of challenges that are relevant in the context of network fuzzing.

\textbf{State space exploration:} In conventional binary fuzzing targets the code coverage was the most important guiding metric. Network applications on the other hand, function according to a protocol-specific state machine. In contrast to stateless applications, the protocol state is too important to ignore in favor of only code coverage. Doing so has the potential of degrading performance to the level of a random fuzzer. Thus, the first problem becomes identifying the protocol state space of the target. One approach would be to implement a similar state machine based on the protocol specification, then correlate the SUT output with state transitions. This may prove difficult for multiple reasons. First fully implementing a protocol that has been revised numerous times over a period of dozens of years should most likely be considered impractical. Second, this protocol specification may not even be publicly available for proprietary firmware. As a result, the developer may first have to reverse engineer the protocol, which is a challenge in and of itself. Finally, this solution is not scalable. For example, the authors of a recent network fuzzing benchmark \cite{natella2021profuzzbench} had to implement protocol decoders in order to test \texttt{AFLNet} on their chosen servers. Another approach is to attempt identifying the internal state in a protocol agnostic manner. While this could be achieved, the accuracy is highly dependent on the manner in which the target application stores its session state. In this case, some success may be had by employing automatic protocol reverse engineering techniques \cite{gascon2015pulsar} that have been previously explored in black-box fuzzing.

\textbf{System state:} Maybe more so than other types of applications, network server operation very much depends of the state of the entire system. During its normal operation, the network application interacts with other components such as third party processes (e.g., a database) or the filesystem (e.g., SSH client checking server fingerprint in \texttt{\~{}/.ssh/known\_hosts}). This becomes a problem when considering predictability is paramount in fuzzing. If one is unable to reproduce a code path, or even a crash with the same input file, the whole process becomes an exercise in futility. Consequently, a major challenge is ensuring that the fuzzer controls all external influencers for a given SUT.

\textbf{High overhead:} One of the deciding factors when it comes to fuzzer performance is execution speed. In the past, many improvements have been made in order to maximize the number of executions per second. Nonetheless, network applications are distinctly slow. First of all, many applications automatically read configuration data from well-known files. Second, all data transmitted between client and server usually traverses the network kernel stack, which is known to be a bottleneck \cite{hoiland2018express,scholz2014look,kuenzer2021unikraft}. Especially when the session must be renegotiated for each particular testcase. Finally, network applications can perform Inter Process Communication (IPC) operations in order to satisfy a client request, either between instances of the same program or by accessing a database. We believe the main focus should be on bypassing the kernel network stack. Meanwhile, the option of manually configuring the SUT in order to limit reliance on third party systems should be investigated on a case-by-case basis. If this is identified as a significant bottleneck, instrumentation-assisted bypasses could sometimes be a valid option.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Survey of recent network fuzzers}
\label{extend:netfuzz:survey}

\subsubsection{AFLNet}
\label{extend:netfuzz:survey:aflnet}

\texttt{AFLNet} \cite{pham2020aflnet} is the first \texttt{AFL} variant to specialize in network server fuzzing. In addition to code coverage, \texttt{AFLNet} utilizes protocol state information to guide its input generation strategy. Its main advantage over previous stateful coverage guided fuzzers consists in its ability to infer the protocol state machine based on runtime observations. In their evaluation, the authors demonstrated that their inference technique is compatible with FTP and RTSP. \texttt{AFLNet} uses packet captures as seeds to extract data exchanges and make initial assumptions regarding a typical response pattern from the server. The data is delivered using the POSIX socket interface, an extension added to the original \texttt{AFL}. Although a sound delivery strategy, there is a significant overhead that can be attributed to the data crossing the kernel boundary and passing through the network stack.

\subsubsection{StateAFL}
\label{extend:netfuzz:survey:stateafl}

Based on \texttt{AFLNet}, \texttt{StateAFL} proposes a number of improvements to the 2020 stateful network fuzzer. The most notable contribution that we identified is its alternative protocol state inference system. In contrast to \texttt{AFLNet}, this system is based not on server replies, but in stead on hashes of long-lived memory areas. Natella remarks that the SUT must maintain the protocol state of each session in a data structure. Moreover, the lifespan of this data structure should exceed the duration of a single request-reply exchange but not that of the session itself. The lifespan of such structures are also done through instrumentation, by hooking the dynamic memory allocation API. After each request-reply sequence (also identified through static instrumentation), the long-lived memory sections are labeled via locality-based hashing \cite{jafari2021survey}. This is necessary because hash functions are otherwise sensitive to small changes in their input. It would not be unreasonable to think that the session state structures would contain pointers to memory buffers, and that these buffers would need to be resized at certain points. The author notes that disabling randomization mechanisms such as ASLR alleviates this problem to a certain degree. This improvement over \texttt{AFLNet} makes it unnecessary to interpret the reply messages of the server. 

\subsubsection{Nyx-Net}
\label{extend:netfuzz:survey:nyxnet}

\texttt{Nyx-Net} \cite{schumilo2022nyx} is a hypervisor assisted fuzzer that extends \texttt{Nyx} \cite{schumilo2021nyx} by adding adding client-side protocol stack emulation support and payload delivery to a target server via sockets. This solution executes the target process inside a Virtual Machine (VM), using modified versions of \texttt{QEMU} and \texttt{KVM}. The reason for this is twofold:

\begin{itemize}
    \item \textit{Use of incremental snapshots:} By leveraging the snapshot mechanisms available to most VM Managers, it is able to reset the internal state of not only the userspace server process, but also of the kernel network stack. Moreover, this allows the fuzzer to skip large amounts of packet exchanges that are required for configuration purposes (e.g., handshakes, ciphersuite negotiations, authentication, etc.) While always maintaining a root snapshot (i.e., initial state of the process), the authors state that retaining more than one other snapshot would negatively impact performance. Their implementation tracks guest physical pages that are dirtied following the creation of a snapshot. Although this drastically reduces the amount of pages that need to be stored for future restoration, there is an argument to be made that guests-side TLB misses may render extensive use of this technique impractical. In stead, Schumilo et al. concluded that revisiting uninterested code paths would be preferable to overreliance on the snapshot mechanism.

    \item \textit{Efficient environment reset:} Target programs can often perform changes to the entire system. Moreover, this changes can affect subsequent executions of the same target. A few examples of such changes that are relevant to network servers are configuration files and databases. \texttt{Nyx-Net} implements the snapshot mechanisms not only for main memory, but also for block device sectors. As such, reverting to a previous state in preparation of evaluating a new input also solves the problem of cleaning the environment.
\end{itemize}

The execution unit of \texttt{Nyx-Net} relies on \texttt{libc} system call hooking using the dynamic loader \texttt{LD\_PRELOAD} override variable. By replacing the socket interaction interface (e.g., \texttt{sendmsg()}, \texttt{dup2()}, \texttt{epoll()}, etc.), the fuzzer is able to bypass the network stack and send data directly to the target process via shared memory. Moreover, the target binaries are not instrumented. In stead, \texttt{Nyx-Net} relies on the Intel Processor Trace extension to determine the control flow of the analyzed process.

The fuzzer also comes with a companion application written in Python that is able to synthesize application data payloads from network captures. These payloads are transmitted to the \texttt{LD\_PRELOAD} agent as a series of instructions with optional data arguments. One difficulty that the authors identified when discarding the network stack is that the content of a single packet does not necessarily correspond to the data transferred to or from userspace via a system call. To solve this, the authors used packet dissectors similar to those of \texttt{AFLNet} in order to establish packet boundaries.

\subsubsection{HyperDbg}
\label{extend:netfuzz:survey:hyperdbg}

Although not specifically a network fuzzer, \texttt{HyperDbg} \cite{karvandi2022hyperdbg} is similar to \texttt{Nyx} and \texttt{Nyx-Net} in that it leverages hypervisor-level privileges to more efficiently gain control over the SUT. Although it specializes on malware analysis, \texttt{HyperDbg} is also capable of performing kernel fuzzing. This not only allows the avoidance of failures that may lead to system resets, but also permits concentrated fuzzing of certain interfaces, akin in design to the in-process LLVM LibFuzzer \cite{chao2018design}. If \texttt{Nyx-Net} optimality stems in part from its network stack bypass, \texttt{HyperDbg} could serve a complementary role through the fuzzing of network-related kernel modules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmarks}
\label{extend:netfuzz:benchmarks}

One of the main challenges in this area of study today is deciding how to express the relative performance of different fuzzers. The academic community has struggled for many years to reach a consensus regarding a methodology or set of tools for fuzzer evaluation. Following are the most relevant examples of benchmarking frameworks originating both from academic research and industry-driven necessity. 

\subsubsection{Google OSS-Fuzz \& fuzzbench}
\label{extend:netfuzz:benchmarks:fuzzbench}

At the time when OSS-Fuzz \cite{serebryany2017oss} was made publicly available, there was no infrastructure for automated, continuous fuzzing. In 2016, Google took a first step in this direction by releasing their internal fuzzing-based testing framework for Chrome. This new testing service allowed contributions both in terms of additional fuzzers but arguably more important, Open Source projects to be evaluated. Although individuals were able to develop and run forks of the core fuzzing engine (i.e., ClusterFuzz \cite{clusterfuzz}) locally, the fact that OSS-Fuzz would perform continuous testing while utilizing Google compute resources made large scale fuzzing more accessible both for researchers and developers. According to a recent report \cite{chang_2023}, this lead to the discovery of 8,800 vulnerabilities and 28,000 bugs across the past seven years. However, we note that according to their responsible disclosure policy, newly detected bugs would first be reported to the maintainer of the related project and made publicly known only thirty days after a fix was emitted or ninety days after the report was first dispatched. While this project promoted wide adoption of better security standards, it could not be considered fit as a fuzzer benchmarking platform.

In response to this shortcoming of OSS-Fuzz, in 2020 Google released FuzzBench \cite{fuzzbench,metzman2021fuzzbench}, a coverage based evaluation platform dedicated not to uncovering bugs, but to providing a fair evaluation of state of the art fuzzers. This service addressed the concern that at the time, evidence in support of new fuzzers was insufficient \cite{klees2018evaluating} to back their claims pertaining to performance on real-world targets. As opposed to OSS-Fuzz, while researchers were able to freely contribute new fuzzers, the set of target applications was heavily curated. The reason for this was to ensure a wide variety of input formats and functionality. In turn, this would ensure a certain degree of protection against overfitting. Although code coverage is widely accepted as an adequate metric, additional research \cite{asprone2022comparing} based on FuzzBench has also incorporated the amount of detected bugs into the final evaluation score. In addition to performing a comparative analysis between fuzzers, we note that FuzzBench has also been used to perform in-depth feature analyses \cite{fioraldi2023dissecting} of single fuzzers.

\subsubsection{LAVA}
\label{extend:netfuzz:benchmarks:lava}

LAVA \cite{dolan2016lava} is a technique based on taint analysis that is able to insert artificial bugs in the source code of existing software. By creating a synthetic corpus of bugs (i.e., LAVA-M), Dolan-Gavit et al. proposed a departure from the widely accepted code coverage-based evaluation, and a return to a focus on bug discovery. Their solution traces input data propagation during the execution of a SUT and identifies regions where certain input bytes have been minimally altered. Although these bytes represent prime candidates as potential synthetic bug triggers, they must also not influence the control flow of the program. The bugs inserted in the program are exclusively triggered by magic value comparisons. The authors created two bug corpora using LAVA: LAVA-1 and LAVA-M. The former consists of single bugs inserted in \texttt{binutils} tools, as well as \texttt{bash} and \texttt{tshark}. The latter is more widely used in academic research and incorporates multiple bugs across four \texttt{coreutils} tools. Both corpora have been tested against a coverage-guided fuzzer and a symbolic execution engine that also incorporates a SAT solver (note that neither are identified by name in the paper).

\subsubsection{MAGMA}
\label{extend:netfuzz:benchmarks:magma}

MAGMA \cite{hazimeh2020magma} is a ground truth open fuzzing benchmark. By placing emphasis on bug discovery as the main evaluation metric, Hazimeh et al. contribute a set of seven real world targets with 118 bugs. Additionally, they provide 25 wrapper programs that execute the core seven targets while exercising different features. The bugs inserted in these targets differ from those generated by LAVA-M in that they are derived from older versions of the SUTs, thus lending them more credibility. The authors have manually selected these bugs, introduced them in the latest versions of the targets and added code meant to verify a crash condition. This allows them to simulate the LAVA-M behaviour of crashing the SUT immediately if the crash condition has been satisfied. Alternatively, they can continue execution without triggering the bug but noting that the condition has been satisfied. This canary code has the benefit of not having to compile the benchmark programs with sanitizers, thus reducing the overhead and maximizing the number of executions per second. By dedicating over 200,000 CPU-hours, the authors evaluate MAGMA against eight state of the art fuzzers. They confirmd that after a 24h long experiment, only 62\% of bugs were identified.

\subsubsection{ProtoFuzzBench}
\label{extend:netfuzz:benchmarks:protofuzzbench}

ProtoFuzzBench \cite{natella2021profuzzbench} is a network fuzzing benchmark focusing on stateful network protocols. Natella et al. seek to address the distinct lack of testing frameworks for protocol fuzzing. To this end, they present a collection of network servers that employ ten different application layer protocols. These range from standardized, well known protocols such as DNS, SSH, FTP to proprietary protocols like DAAP (used by Apple in iTunes). The authors present a set of shell scripts for automated setup and testing of the ten network servers in Docker containers. Additionally, their solution also generates coverage information based on \texttt{gcov}.
